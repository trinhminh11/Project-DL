{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torchsummary\n",
    "# !pip install gymnasium\n",
    "# !pip install gymnasium[box2d]\n",
    "# !pip install pygame\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "# from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "\n",
    "import gym_agent as ga\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 500\n",
    "max_episode_steps = 500\n",
    "chkpt_dir = \"checkpoints/LunarLander-v2/DQN\"\n",
    "\n",
    "env_id = 'LunarLander-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, n_inp: int, n_out: int, features: list[int] = [256, 256]):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [n_inp] + features + [n_out]\n",
    "\n",
    "        self.net = nn.Sequential()\n",
    "\n",
    "        for i in range(len(layers)-1):\n",
    "            self.net.append(nn.Linear(layers[i], layers[i+1]))\n",
    "\n",
    "            if i < len(layers)-2:\n",
    "                self.net.append(nn.ReLU())\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepQNetwork(8, 4)\n",
    "summary(model, (8,), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_inp: int, n_out: int, features: list[int] = [256, 256]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = DeepQNetwork(n_inp, n_out, features)\n",
    "\n",
    "        self.target_network = DeepQNetwork(n_inp, n_out, features)\n",
    "\n",
    "        self.soft_update(1)\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        return self.network(X)\n",
    "    \n",
    "    def target(self, X: torch.Tensor):\n",
    "        return self.target_network(X)\n",
    "    \n",
    "    def soft_update(self, tau: float):\n",
    "        for target_param, param in zip(self.target_network.parameters(), self.network.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(ga.OffPolicyAgent):\n",
    "    policy: Policy\n",
    "    def __init__(\n",
    "            self, \n",
    "            policy, \n",
    "            env, \n",
    "            action_space: list,\n",
    "            lr=1e-3,\n",
    "            gamma = 0.99,\n",
    "            tau = 1e-3,\n",
    "            eps_start = 1.0,\n",
    "            eps_decay = 0.99,\n",
    "            eps_min = 0.01,\n",
    "            batch_size = 64, \n",
    "        ):\n",
    "        super().__init__(policy, env, batch_size=batch_size)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "\n",
    "        self.eps = eps_start\n",
    "\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy.network.parameters(), lr=lr)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.eps = self.eps * self.eps_decay\n",
    "        if self.eps < self.eps_min:\n",
    "            self.eps = self.eps_min\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, observations: np.ndarray, deterministic = True):\n",
    "        if deterministic:\n",
    "            eps = 0\n",
    "        else:\n",
    "            eps = self.eps\n",
    "        \n",
    "        if np.random.random() > eps:\n",
    "            tensor_observations = torch.from_numpy(observations).float().to(self.device)\n",
    "\n",
    "            actions_value: torch.Tensor = self.policy.forward(tensor_observations)\n",
    "\n",
    "            return np.argmax(actions_value.cpu().numpy(), axis=1)\n",
    "        else:\n",
    "            return np.random.choice(self.action_space, observations.shape[0])\n",
    "\n",
    "    def learn(self, observations: torch.Tensor, actions: torch.Tensor, rewards: torch.Tensor, next_observations: torch.Tensor, terminals: torch.Tensor):\n",
    "        q_next: torch.Tensor = self.policy.target(next_observations).detach()\n",
    "\n",
    "        q_next_max = q_next.max(dim=1)[0]\n",
    "\n",
    "        q_target = rewards + self.gamma * q_next_max * (~terminals)\n",
    "\n",
    "        q_expected = self.policy.forward(observations).gather(1, actions.unsqueeze(1).long()).squeeze(1)\n",
    "\n",
    "        # loss = F.mse_loss(q_expected, q_target)\n",
    "        loss = F.smooth_l1_loss(q_expected, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.policy.soft_update(self.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ga.make_vec(env_id, max_episode_steps=max_episode_steps)\n",
    "agent = DQN(\n",
    "    policy=Policy(8, 4, features=[256, 256]),\n",
    "    action_space=[0, 1, 2, 3],\n",
    "    env=env,\n",
    "    lr=1e-3,\n",
    "    batch_size=64\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.fit(n_games=n_games, save_best=True, save_every=100, save_dir=chkpt_dir, progress_bar=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load(chkpt_dir, \"best\")\n",
    "agent.plot_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ga.make(env_id, render_mode='rgb_array')\n",
    "\n",
    "agent.load(chkpt_dir, \"best\")\n",
    "\n",
    "agent.play_jupyter(env, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- using Deep Q Learning to solve LunarLander with observation is iamge (96, 96, 3) (below env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "class ImageAsObs(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(96, 96, 3), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        image = self.env.unwrapped.render()\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((96, 96))\n",
    "        return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ga.make(env_id, render_mode='rgb_array')\n",
    "env.add_wrapper(ImageAsObs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
