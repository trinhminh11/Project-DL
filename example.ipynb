{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import random\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Local application imports\n",
    "from agent_base import AgentBase\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, n_inp: int, features: list[int], n_actions: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a list of layer sizes including input, hidden, and output layers\n",
    "        layer_sizes = [n_inp] + features + [n_actions]\n",
    "\n",
    "        # Initialize an empty sequential container\n",
    "        self.net = nn.Sequential()\n",
    "\n",
    "        # Loop through the layer sizes to create the network\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Add a linear layer\n",
    "            self.net.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            \n",
    "            # Add ReLU activation function for all layers except the last one\n",
    "            if i != len(layer_sizes) - 2:\n",
    "                self.net.append(nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        # Pass the input state through the network\n",
    "        return self.net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "checkout https://www.youtube.com/watch?v=wc-FxNENg9U&t\n",
    "this is the implementation of that tutorial\n",
    "while the tutorial dont implement Q-target Network\n",
    "this is the tutorial that implement Q-target Network:\n",
    "https://goodboychan.github.io/python/reinforcement_learning/pytorch/udacity/2021/05/07/DQN-LunarLander.html\n",
    "\n",
    "The target network is a separate neural network that is used to estimate the target values\n",
    "for the Q-learning update rule. It is a copy of the main network, but its parameters are \n",
    "updated less frequently, which helps stabilize the learning process.\n",
    "\n",
    "Using a single neural network for both estimating the current Q-values and updating the \n",
    "target Q-values can lead to instability in the learning process. This is because the \n",
    "networkâ€™s parameters are constantly changing, causing the target values to shift as well. To \n",
    "address this issue, the concept of a target network is introduced.\n",
    "\n",
    "The target network is a separate neural network that is periodically updated with the \n",
    "parameters of the main Q-network. This means that the target values used for the Q-learning \n",
    "update rule remain more stable, allowing for a more stable learning process. For example, \n",
    "consider a reinforcement learning problem where an agent is learning to navigate a maze.\n",
    "The agent uses a Q-network to estimate the Q-values for each possible action in its current \n",
    "state. To update the Q-values, the agent also needs to estimate the target Q-values for the \n",
    "next state. Instead of using the same Q-network for this purpose, the agent uses a separate \n",
    "target network, which is updated less frequently. This helps stabilize the learning process \n",
    "and allows the agent to learn more effectively.\n",
    "\n",
    "In summary, a target network is a separate neural network used in deep reinforcement learning \n",
    "algorithms to stabilize the learning process. It is a copy of the main Q-network, but its \n",
    "parameters are updated less frequently, providing more stable target values for the Q-learning \n",
    "update rule.\n",
    "'''\n",
    "\n",
    "class DQN(AgentBase):\n",
    "    def __init__(\n",
    "            self,\n",
    "            gamma: float,\n",
    "            lr: float,\n",
    "            state_shape: int,\n",
    "            action_shape: int,\n",
    "            action_space: int,\n",
    "            batch_size: int,\n",
    "            update_every: int,\n",
    "            eps_start: float = 1.0,\n",
    "            eps_decay: float = 0.995,\n",
    "            eps_end: float = 0.01,\n",
    "            max_mem_size: int = 100000,\n",
    "            device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            seed: int = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a DQN agent.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            gamma (float): Discount factor for future rewards\n",
    "            lr (float): Learning rate for the optimizer\n",
    "            state_shape (int): Dimension of each state\n",
    "            action_shape (int): Dimension of each action\n",
    "            batch_size (int): Size of each training batch\n",
    "            update_every (int): How often to update the network\n",
    "            eps_start (float): Starting value of epsilon, for epsilon-greedy action selection\n",
    "            eps_decay (float): Multiplicative factor (per episode) for decreasing epsilon\n",
    "            eps_end (float): Minimum value of epsilon\n",
    "            max_mem_size (int): Maximum size of the replay buffer\n",
    "            device (str): Device to use for tensor computations ('cpu' or 'cuda')\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super().__init__(state_shape, action_shape, batch_size, max_mem_size, update_every, device, seed)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape,\n",
    "        self.action_space = action_space\n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_end = eps_end\n",
    "        self.max_mem_size = max_mem_size\n",
    "        self.device = device\n",
    "        self.seed = seed\n",
    "\n",
    "        # Number of input features\n",
    "        self.n_inp = state_shape[0]\n",
    "        # Memory counter\n",
    "        self.mem_cntr = 0\n",
    "        # Soft update parameter\n",
    "        self.tau = 1e-3\n",
    "\n",
    "        # Local model for action value estimation\n",
    "        self.local_model = DeepQNetwork(n_inp=self.n_inp, features=[256, 256], n_actions=len(action_space)).to(device)\n",
    "        # Target model for action value estimation\n",
    "        self.target_model = DeepQNetwork(n_inp=self.n_inp, features=[256, 256], n_actions=len(action_space)).to(device)\n",
    "        # Optimizer for the local model\n",
    "        self.optimizer = optim.Adam(self.local_model.parameters(), lr)\n",
    "\n",
    "        # Initialize time step for updating every 'update_every' steps\n",
    "        self.time_step = 0\n",
    "        # Initialize epsilon for epsilon-greedy policy\n",
    "        self.eps = eps_start\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the epsilon value by decaying it according to the epsilon decay rate.\n",
    "        \n",
    "        The epsilon value is updated to be the maximum of the end epsilon value and \n",
    "        the product of the current epsilon value and the epsilon decay rate.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.eps = max(self.eps_end, self.eps_decay * self.eps)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Select an action for the given state using an epsilon-greedy policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (np.ndarray): Current state\n",
    "\n",
    "        Returns\n",
    "        =======\n",
    "            action (int): Action to be taken\n",
    "        \"\"\"\n",
    "        # Determine epsilon value based on evaluation mode\n",
    "        if self.eval:\n",
    "            eps = 0\n",
    "        else:\n",
    "            eps = self.eps\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() >= eps:\n",
    "            # Convert state to tensor and move to the appropriate device\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "\n",
    "            # Set local model to evaluation mode\n",
    "            self.local_model.eval()\n",
    "            # Get action values from the local model\n",
    "            action_value = self.local_model(state)\n",
    "            # Set local model back to training mode\n",
    "            self.local_model.train()\n",
    "\n",
    "            # Return the action with the highest value\n",
    "            return np.argmax(action_value.cpu().data.numpy())\n",
    "        else:\n",
    "            # Return a random action from the action space\n",
    "            return random.choice(self.action_space)\n",
    "\n",
    "    def learn(self, states: Tensor, actions: Tensor, rewards: Tensor, next_states: Tensor, terminals: Tensor):\n",
    "        \"\"\"\n",
    "        Update the value network using a batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            states (Tensor): Batch of current states\n",
    "            actions (Tensor): Batch of actions taken\n",
    "            rewards (Tensor): Batch of rewards received\n",
    "            next_states (Tensor): Batch of next states\n",
    "            terminals (Tensor): Batch of terminal flags indicating episode end\n",
    "        \"\"\"\n",
    "        # Get the maximum predicted Q values for the next states from the target model\n",
    "        q_targets_next = self.target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        # Compute the Q targets for the current states\n",
    "        q_targets = rewards + (self.gamma * q_targets_next * (~terminals))\n",
    "\n",
    "        # Get the expected Q values from the local model\n",
    "        q_expected = self.local_model(states).gather(1, actions.long())\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update the target network\n",
    "        self.soft_update()\n",
    "\n",
    "    def soft_update(self):\n",
    "        for target_param, local_param in zip(self.target_model.parameters(), self.local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games, max_t = 500, 1000\n",
    "chkpt_dir = 'checkpoint/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = utils.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN(\n",
    "    gamma=0.99,\n",
    "    batch_size=64,\n",
    "    state_shape=(8,),\n",
    "    action_shape=(1,),\n",
    "    action_space=[0,1,2,3],\n",
    "    max_mem_size=int(1e5),\n",
    "    update_every=1,\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    lr=5e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(agent.local_model, (8, ), 64, agent.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = agent.fit(env, n_games, max_t, save_best=True, save_last=False, save_dir = 'checkpoint/', progress_bar=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plotting(np.arange(n_games), scores = scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = utils.make('LunarLander-v2', render_mode = 'human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load('checkpoint/DQN')\n",
    "\n",
    "agent.play(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply transform to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new environment instance\n",
    "env = utils.make('LunarLander-v2')\n",
    "\n",
    "# Define a custom reward transformation class\n",
    "class RewardTransform(utils.Transform):\n",
    "    def __init__(self, time_penalty = -0.0001) -> None:\n",
    "        super().__init__()\n",
    "        self.time_penalty = time_penalty\n",
    "        self.t = 0\n",
    "    \n",
    "    def __call__(self, reward):\n",
    "        # Increment time step and apply time penalty to the reward\n",
    "        self.t += 1\n",
    "        return reward + self.time_penalty * self.t\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the time step counter\n",
    "        self.t = 0\n",
    "\n",
    "# Instantiate the reward transformation\n",
    "reward_tfm = RewardTransform()\n",
    "\n",
    "# Apply the reward transformation to the environment\n",
    "env.set_reward_transform(reward_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN(\n",
    "    gamma=0.99,\n",
    "    batch_size=64,\n",
    "    state_shape=(8,),\n",
    "    action_shape=(1,),\n",
    "    action_space=[0,1,2,3],\n",
    "    max_mem_size=int(1e5),\n",
    "    update_every=1,\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    lr=5e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = agent.fit(env, n_games, max_t, save_best=True, save_last=False, save_dir='checkpoint/', progress_bar=tqdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "although this reward look bad than these above\n",
    "\n",
    "due to the reward transform, but it's actually make the agent play faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plotting(np.arange(n_games), scores = scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = utils.make('LunarLander-v2', render_mode = 'human')\n",
    "\n",
    "env.set_reward_transform(reward_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load('checkpoint/DQN')\n",
    "\n",
    "agent.play(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
