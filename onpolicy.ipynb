{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import random, os\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tqdm\n",
    "from numpy._typing import _ShapeLike\n",
    "\n",
    "# Local application imports\n",
    "from agent_base import AgentBase\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, n_inp, n_actions, features = [128, 128]):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        layer_sizes = [n_inp] + features\n",
    "\n",
    "        self.encoded = nn.Sequential()\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.encoded.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            self.encoded.append(nn.ReLU(inplace=True))\n",
    "        self.critic = nn.Linear(layer_sizes[-1], 1)\n",
    "        \n",
    "        self.actor = nn.Linear(layer_sizes[-1], n_actions)\n",
    " \n",
    "    def forward(self, X: Tensor):\n",
    "        encoded = self.encoded(X)\n",
    "        return self.critic(encoded), self.actor(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(AgentBase):\n",
    "    def __init__(\n",
    "            self, \n",
    "            state_shape: _ShapeLike, \n",
    "            action_shape: _ShapeLike, \n",
    "            n_actions: int,\n",
    "            lr = 3e-5,\n",
    "            gamma = 0.99,\n",
    "            batch_size: int = 64, \n",
    "            max_mem_size: int = int(1e5),  \n",
    "            update_every: int = 1, \n",
    "            device: str = 'cuda' if torch.cuda.is_available() else 'cpu', \n",
    "            seed = 0, \n",
    "            **kwargs\n",
    "        ) -> None:\n",
    "        super().__init__(state_shape, action_shape, batch_size, max_mem_size, update_every, device, seed, **kwargs)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "        self.actor_critic = ActorCriticNetwork(state_shape[0], n_actions).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def act(self, state: np.ndarray):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        _, action_probs = self.actor_critic(state)[0]\n",
    "        action_probs = F.softmax(action_probs, dim=0)\n",
    "        action = torch.distributions.Categorical(action_probs).sample()\n",
    "        self.action = action\n",
    "        return action.item()\n",
    "    \n",
    "    def learn(self, state: Tensor, reward: Tensor, next_state: Tensor, terminal: Tensor):\n",
    "\n",
    "        action_value, probs = self.actor_critic(state)[0]\n",
    "        next_action_value, _ = self.actor_critic(next_state)[0]\n",
    "\n",
    "        action_probs = F.softmax(probs, dim=0)\n",
    "        action_probs = torch.distributions.Categorical(action_probs)\n",
    "        log_prob: Tensor = action_probs.log_prob(self.action)\n",
    "\n",
    "        delta = reward + self.gamma * next_action_value * (~terminal) - action_value\n",
    "        actor_loss = -log_prob * delta\n",
    "        critic_loss = delta.pow(2)\n",
    "\n",
    "        total_loss = actor_loss + critic_loss\n",
    "\n",
    "        self.actor_critic.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = utils.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ActorCritic(\n",
    "    state_shape = env.observation_space.shape,\n",
    "    action_shape = (1, ),\n",
    "    n_actions = env.action_space.n,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 1000\n",
    "scores = []\n",
    "for i in tqdm(range(n_games)):\n",
    "    done = False\n",
    "    state = env.reset()[0]\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated\n",
    "\n",
    "        agent.learn(state, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "    scores.append(score)\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print(f'Episode: {i}, Score: {score}, Avg Score: {avg_score}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
