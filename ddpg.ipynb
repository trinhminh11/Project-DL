{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, device\n",
    "\n",
    "import utils\n",
    "\n",
    "from agent import AgentBase\n",
    "\n",
    "from numpy._typing import _ShapeLike\n",
    "\n",
    "from typing import Type, Any\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Actor(nn.Module):\n",
    "#     def __init__(self, inp_channels: int, n_actions: int, features = [16, 32, 64, 128]):\n",
    "#         super().__init__()\n",
    "#         self.inp_channels = inp_channels\n",
    "#         self.n_actions = n_actions\n",
    "#         self.features = features\n",
    "\n",
    "#         self.initial = nn.Sequential(\n",
    "#             nn.Conv2d(inp_channels, features[0], 3, 1, 1),\n",
    "#             nn.ReLU(True)\n",
    "#         )        \n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#             *[utils.ConvBn(features[i], features[i+1], pool = (i!=(len(features)-2))) for i in range(len(features)-1)]\n",
    "#         )\n",
    "\n",
    "#         self.net.append(nn.AdaptiveMaxPool2d((1, 1)))\n",
    "#         self.net.append(nn.Flatten())\n",
    "\n",
    "#         self.mu = nn.Sequential(\n",
    "#             nn.Linear(features[-1], 64),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(64, n_actions),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, states: Tensor) -> Tensor:\n",
    "#         states_encoder = self.net(self.initial(states))\n",
    "\n",
    "#         mu = self.mu(states_encoder)\n",
    "\n",
    "#         return mu\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_inp: int, n_action, features = [64, 64]):\n",
    "        super().__init__()\n",
    "\n",
    "        layer_sizes = [n_inp] + features\n",
    "\n",
    "        self.encoded = nn.Sequential()\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.encoded.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            self.encoded.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.actor = nn.Linear(layer_sizes[-1], n_action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.tanh(self.actor(self.encoded(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Critic(nn.Module):\n",
    "#     def __init__(self, inp_channels: int, n_actions: int, features = [16, 32, 64, 128]):\n",
    "#         super().__init__()\n",
    "#         self.inp_channels = inp_channels\n",
    "#         self.n_actions = n_actions\n",
    "#         self.features = features\n",
    "\n",
    "#         self.initial = nn.Sequential(\n",
    "#             nn.Conv2d(inp_channels, features[0], 3, 1, 1),\n",
    "#             nn.ReLU(True)\n",
    "#         )        \n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#             *[utils.ConvBn(features[i], features[i+1], pool = (i!=(len(features)-2))) for i in range(len(features)-1)]\n",
    "#         )\n",
    "\n",
    "#         self.net.append(nn.AdaptiveMaxPool2d((1, 1)))\n",
    "#         self.net.append(nn.Flatten())\n",
    "\n",
    "#         self.q = nn.Sequential(\n",
    "#             nn.Linear(features[-1] + n_actions, 64),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(32, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, states: Tensor, action: Tensor) -> Tensor:\n",
    "#         states_encoder = self.net(self.initial(states))\n",
    "\n",
    "#         action_value = self.q(torch.concat([states_encoder, action], 1))\n",
    "\n",
    "#         return action_value\n",
    "    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_inp, n_action, features = [64, 64]):\n",
    "        super(Critic, self).__init__()\n",
    "        layer_sizes = [n_inp + n_action] + features\n",
    "\n",
    "        self.encoded = nn.Sequential()\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.encoded.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            self.encoded.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.critic = nn.Linear(layer_sizes[-1], 1)\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        return self.critic(self.encoded(torch.concat([states, actions], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(AgentBase):\n",
    "    def __init__(\n",
    "            self, \n",
    "            state_shape: _ShapeLike, \n",
    "            action_shape: _ShapeLike, \n",
    "            batch_size: int, \n",
    "            actor_lr = 1e-3,\n",
    "            critic_lr = 2e-3,\n",
    "            actor_optim: Type[optim.Optimizer] = optim.Adam,\n",
    "            critic_optim: Type[optim.Optimizer] = optim.Adam,\n",
    "            gamma = 0.99,\n",
    "            tau = 5e-3,\n",
    "            noise = 0.1,\n",
    "            buffer_size: int = int(1e5), \n",
    "            update_every: int = 1, \n",
    "            device: str = 'cuda' if torch.cuda.is_available() else 'cpu', \n",
    "            seed = 0, \n",
    "            **kwargs\n",
    "        ) -> None:\n",
    "\n",
    "        super().__init__(state_shape, action_shape, batch_size, False, update_every, buffer_size, device, seed, **kwargs)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.noise = noise\n",
    "\n",
    "        self.actor = Actor(state_shape[0], n_action=action_shape[0]).to(device)\n",
    "        self.critic = Critic(state_shape[0], action_shape[0]).to(device)\n",
    "        self.target_actor = Actor(state_shape[0], n_action=action_shape[0]).to(device)\n",
    "        self.target_critic = Critic(state_shape[0], action_shape[0]).to(device)\n",
    "\n",
    "        self.actor_optim = actor_optim(self.actor.parameters(), actor_lr)\n",
    "        self.critic_optim = critic_optim(self.critic.parameters(), critic_lr)\n",
    "\n",
    "        self.apply(utils.init_weights)\n",
    "\n",
    "        self.update_network_parameters(tau = 1)\n",
    "\n",
    "    def update_network_parameters(self, tau = None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        \n",
    "        for target_param, local_param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "        \n",
    "        for target_param, local_param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state: np.ndarray) -> Any:\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "\n",
    "        self.actor.eval()\n",
    "        action: np.ndarray = self.actor(state)[0].cpu().detach().numpy()\n",
    "        self.actor.train()\n",
    "\n",
    "        if not self.eval:\n",
    "            action += np.random.normal(0, self.noise, action.shape).clip(-1, 1)\n",
    "        \n",
    "        return action\n",
    "\n",
    "        # return np.array([action[0], action[1], 0]) if action[1]>0 else np.array([action[0], 0, -action[1]])\n",
    "\n",
    "    def learn(self, states: Tensor, actions: Tensor, rewards: Tensor, next_states: Tensor, terminals: Tensor):\n",
    "        self.eval = False\n",
    "        target_actions = self.target_actor(next_states)\n",
    "        next_q = self.target_critic(next_states, target_actions)\n",
    "        q_value = self.critic(states, actions)\n",
    "\n",
    "        target_q_value = rewards + self.gamma * next_q * (~terminals)\n",
    "\n",
    "        critic_loss = F.mse_loss(q_value, target_q_value)\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        new_policy_actions = self.actor(states)\n",
    "        actor_loss = -self.critic(states, new_policy_actions).mean()\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        self.update_network_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = utils.make('LunarLanderContinuous-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPG(\n",
    "    state_shape=env.observation_space.shape,\n",
    "    action_shape=env.action_space.shape,\n",
    "    batch_size=64,\n",
    "    actor_lr=0.001,\n",
    "    critic_lr=0.002,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f392983d8fd64d60a2b5741478ee15e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = agent.fit(env, 1000, 1000, save_best=True, save_dir='checkpoint/', progress_bar=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314.00363530131136"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = utils.make('LunarLanderContinuous-v2', render_mode='human')\n",
    "\n",
    "agent.play(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
