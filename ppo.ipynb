{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.11.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym_agent as ga\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "from ppo import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoded(nn.Module):\n",
    "    def __init__(self, n_inp, features):\n",
    "        super().__init__()\n",
    "\n",
    "        layer_sizes = [n_inp] + features\n",
    "\n",
    "        self.encoded = nn.Sequential()\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.encoded.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            self.encoded.append(nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoded(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_inp: int, n_action, features = [128, 128]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoded = Encoded(n_inp, features)\n",
    "        self.actor = nn.Linear(features[-1], n_action)\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        X = F.softmax(self.actor(self.encoded(X)), dim=-1)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_inp: int, features = [128, 128]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoded = Encoded(n_inp, features)\n",
    "        \n",
    "        self.critic = nn.Linear(features[-1], 1)\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        return self.critic(self.encoded(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_inp, n_action, features = [512, 256, 128, 64]):\n",
    "        super().__init__()\n",
    "        self.actor = Actor(n_inp, n_action, features)\n",
    "        self.critic = Critic(n_inp, features)\n",
    "    \n",
    "    def action(self, X: torch.Tensor):\n",
    "        return self.actor(X)\n",
    "    \n",
    "    def value(self, X: torch.Tensor):\n",
    "        return self.critic(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_inp, n_action, features = [512, 256, 128, 64]):\n",
    "        super().__init__()\n",
    "        self.encoded = Encoded(n_inp, features)\n",
    "        \n",
    "        self.actor = nn.Linear(features[-1], n_action)\n",
    "        self.critic = nn.Linear(features[-1], 1)\n",
    "    \n",
    "    def action(self, X: torch.Tensor):\n",
    "        return F.log_softmax(self.actor(self.encoded(X)), dim=-1)\n",
    "    \n",
    "    def value(self, X: torch.Tensor):\n",
    "        return self.critic(self.encoded(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_Lunar(ga.AgentBase):\n",
    "    def __init__(\n",
    "            self, \n",
    "            state_shape, \n",
    "            action_shape, \n",
    "            n_action,\n",
    "            n_epochs,\n",
    "            gamma = 0.99,\n",
    "            lr = 3e-5,\n",
    "            gae_lambda = 0.9,\n",
    "            vf_coef = 0.5,\n",
    "            ent_coef = 0.01,\n",
    "            policy_clip = 0.2,\n",
    "            batch_size = 64, \n",
    "            device = 'cuda', \n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__(state_shape, action_shape, batch_size, True, device=device,**kwargs)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.n_epochs = n_epochs\n",
    "        self.policy_clip = policy_clip\n",
    "        self.vf_coef = vf_coef\n",
    "        self.ent_coef = ent_coef\n",
    "\n",
    "        features = [128, 128]\n",
    "\n",
    "        self.lr = lr\n",
    "\n",
    "        self.policy = Policy(state_shape[0], n_action, features)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr)\n",
    "\n",
    "    def act(self, state: np.ndarray):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "\n",
    "        probs: torch.Tensor = self.policy.action(state)\n",
    "        action = Categorical(probs).sample()\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def learn(self, states: torch.Tensor, actions: torch.Tensor, rewards: torch.Tensor, next_states: torch.Tensor, terminals: torch.Tensor):\n",
    "        states = states.detach().to(self.device)\n",
    "        actions = actions.detach().squeeze(1).to(self.device)\n",
    "        rewards = rewards.detach().squeeze(1).to(self.device)\n",
    "\n",
    "        \n",
    "        # values = self.policy.value(states).squeeze(-1).detach().to(self.device)\n",
    "        old_log_probs = Categorical(self.policy.action(states)).log_prob(actions).detach().to(self.device)\n",
    "\n",
    "        # returns = self.calc_returns(rewards.squeeze(1)).detach().to(self.device)\n",
    "\n",
    "        # advantages: torch.Tensor = self.calc_advantages(returns, values).detach().to(self.device)\n",
    "\n",
    "        advantages, returns = self.calc_advantages_and_returns(states, rewards, terminals, next_states[-1])\n",
    "\n",
    "\n",
    "        for _ in range(self.n_epochs):\n",
    "\n",
    "            critic_value: torch.Tensor = self.policy.value(states).squeeze(1)\n",
    "            new_action_probs: torch.Tensor = self.policy.action(states)\n",
    "\n",
    "            new_log_probs: torch.Tensor = Categorical(new_action_probs).log_prob(actions)\n",
    "\n",
    "            prob_ratio = (new_log_probs - old_log_probs).exp()\n",
    "\n",
    "            #prob_ratio = (new_probs - old_probs).exp()\n",
    "            weighted_probs = prob_ratio*advantages\n",
    "\n",
    "            weighted_clipped_probs = torch.clamp(prob_ratio, 1-self.policy_clip,\n",
    "                    1+self.policy_clip)*advantages\n",
    "            \n",
    "            actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "            critic_loss = F.smooth_l1_loss(returns, critic_value).mean()\n",
    "\n",
    "            total_loss = actor_loss + self.vf_coef*critic_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            # actor_loss.backward()\n",
    "            # critic_loss.backward()\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        \n",
    "    def calc_advantages_and_returns(self, states: torch.Tensor, rewards: torch.Tensor, terminals: bool, last_state: torch.Tensor):\n",
    "        if self.gae_lambda == 1:\n",
    "            returns = self.calc_returns(rewards).detach().to(self.device)\n",
    "            values = self.policy.value(states).squeeze(-1).detach().to(self.device)\n",
    "            advantages = self.calc_advantages(returns, values).detach().to(self.device)\n",
    "\n",
    "            return advantages, returns\n",
    "\n",
    "        T = len(states)\n",
    "        \n",
    "        values: torch.Tensor = self.policy.value(torch.cat([states, last_state.unsqueeze(0)])).squeeze(-1)\n",
    "\n",
    "        advantages = torch.zeros((T, ), device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(T)):\n",
    "                delta = rewards[t] - values[t] + self.gamma * values[t+1] * (~terminals[t])\n",
    "\n",
    "                advantages[t] = lastgaelam = delta + self.gamma * self.gae_lambda * lastgaelam * (~terminals[t])\n",
    "\n",
    "            advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "            returns = advantages + values[:-1]\n",
    "            \n",
    "            returns = (returns - returns.mean()) / returns.std()\n",
    "\n",
    "        return advantages, returns\n",
    "    \n",
    "    def calc_returns(self, rewards):\n",
    "        returns = torch.zeros_like(rewards).to(self.device)\n",
    "        R = 0\n",
    "\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = rewards[i] + R * self.gamma\n",
    "            returns[i] = R\n",
    "            \n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "            \n",
    "        return returns\n",
    "\n",
    "    def calc_advantages(self, returns, values):\n",
    "        advantages = returns - values\n",
    "        return (advantages - advantages.mean()) / advantages.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO_Lunar(\n",
    "    state_shape=(8, ),\n",
    "    action_shape=(1, ),\n",
    "    n_action = 4,\n",
    "    batch_size = None,\n",
    "    n_epochs = 10,\n",
    "    lr = 1e-4,\n",
    "    policy_clip=0.2,\n",
    "    gamma = 0.99,\n",
    "    gae_lambda=1\n",
    ")\n",
    "\n",
    "agent.apply(ga.init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ga.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddccd1af57b4b9b829eb638a73d1ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = agent.fit(env, 1000, 500, True, False, save_dir='checkpoints/PPO_Lunar', progress_bar = tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.476866666118724"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = ga.make('LunarLander-v2', render_mode='human')\n",
    "agent.load('checkpoints/PPO_Lunar')\n",
    "agent.play(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, DQN\n",
    "\n",
    "a = PPO()\n",
    "\n",
    "a.learn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
